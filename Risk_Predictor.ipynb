{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Insight to What I have done :- \n",
    "# My first response was to try a motion detector . I then realised that there are way too many people and also other \n",
    "# stochasticities like moving cars and such. I tried HOG as well, but it gave worse results . Finally I used the yolov3 model\n",
    "# for predictions. Since I have only one camera video,there is no way for me to obectively determine safe distance because \n",
    "# a) images do not offer depth perception\n",
    "# b) Objects closer to camera will show more pixel to pixel distance than they actually have. So I have currently set a threshold based on a rough estimate based on trying different values.\n",
    "\n",
    "# Kindly unzip the file in the link . You will find 2 videos, 1 for motion detection and other using YOLO. My main submission in the YOLO one.\n",
    "# The main video(named output.mp4) carries the results. I did not include it in the notebook because it did not make much sense without the corresponding images.\n",
    "# The video is very fast so you can either pause the video and look at the values or run it at 0.5 or 0.75x.\n",
    "# In the main video,you will be able to see real time (w.r.t to video) number of people and no of people [among those the code detected] at risk who will also be marked in red. \n",
    "# The motion detection video is included just as it is a baseline. Do note that the  number of people detected can be increased at the risk of some incorrect detections simultaneously.\n",
    "# Currently I have tried to keep number of incorrects at minimum instead of maximizing no of correct.\n",
    "\n",
    "# NOTE :- If you want to run the code yourself, you will have to put the .cfg,.weights and .names fill in the same directory (Can be found in the zip file) . Also,the videos should be there.\n",
    "\n",
    "# To understand the code, you can follow the comments :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'release'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-feac668f7dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'release'"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture('Video.mp4')\n",
    "ret,frame1 = capture.read()\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = None\n",
    "while(capture.isOpened()):\n",
    "    ret,frame2 = capture.read()\n",
    "    if not ret:\n",
    "        print(0)\n",
    "        break\n",
    "    if(writer == None):\n",
    "        writer = cv2.VideoWriter(\"output_motion_detector.mp4\", fourcc, 30,(frame1.shape[1], frame1.shape[0]), True)\n",
    "    \n",
    "    Difference = cv2.absdiff(frame1,frame2)\n",
    "    gray = cv2.cvtColor(Difference,cv2.COLOR_BGR2GRAY)      \n",
    "    blur = cv2.GaussianBlur(gray,(5,5),1)\n",
    "    \n",
    "    _,thresh = cv2.threshold(blur,20,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "    dilated_image = cv2.dilate(thresh,(5,5),iterations=3)\n",
    "    \n",
    "    contours,_ = cv2.findContours(dilated_image,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) \n",
    "    #cv2.drawContours(frame1,contours,-1,(0,255,0),2)\n",
    "    for contour in contours:\n",
    "        (x,y,w,h) = cv2.boundingRect(contour)\n",
    "        if(7000>=cv2.contourArea(contour) >= 2000):\n",
    "            cv2.rectangle(frame1,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    writer.write(frame1)\n",
    "    cv2.imshow('Image',frame1)\n",
    "    frame1 = frame2\n",
    "    if cv2.waitKey(50) == ord('q'):\n",
    "        break\n",
    "    \n",
    "capture.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindCloserThanThreshold(points,threshold=100):\n",
    "    num = 0\n",
    "    result = []\n",
    "    for (x,y) in points:\n",
    "        check = False\n",
    "        for(a,b) in points:\n",
    "            distance = np.sqrt((x-a)**2 + (b-y)**2)\n",
    "            if(0<distance < threshold):\n",
    "                check = True\n",
    "                num+=1\n",
    "                result.append((x,y))\n",
    "                break\n",
    "        \n",
    "        \n",
    "    return (num,result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Helmet', 'Person', 'Fire', 'Safetyvest']\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Implementing YOLOv3\n",
    "capture = cv2.VideoCapture('Video.mp4')\n",
    "weights_path = \"yolov3_custom_last.weights\"\n",
    "cfg_path = \"yolov3_custom.cfg\"\n",
    "labels_path = \"obj.names\"\n",
    "labels = open(labels_path).read().strip().split(\"\\n\")\n",
    "print(labels)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = None\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(cfg_path, weights_path)\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "while True:\n",
    "    (ret, frame) = capture.read()\n",
    "    if(writer == None):\n",
    "        writer = cv2.VideoWriter(\"output.mp4\", fourcc, 30,(frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # if the frame dimensions are empty, grab them\n",
    "    \n",
    "    (H, W) = frame.shape[:2]\n",
    "\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    #start = time.time()\n",
    "    layerOutputs = net.forward(ln)\n",
    "    #end = time.time()\n",
    "    print('1')\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "    \n",
    "    for output in layerOutputs:\n",
    "        points = [] \n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "            # filter out weak predictions by ensuring the detected\n",
    "            # probability is greater than the minimum probability\n",
    "            if confidence > 0.2 : \n",
    "\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                \n",
    "\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences,0.4,0.5)  # IOU intersection over union\n",
    "\n",
    "    \n",
    "    \n",
    "    if len(idxs) > 0:\n",
    " \n",
    "        for i in idxs.flatten():\n",
    "           \n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            points.append((x+w//2,y+h//2))\n",
    "            color = 120 \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            text = \"{}: {:.4f}\".format(labels[classIDs[i]],\n",
    "            confidences[i])\n",
    "            cv2.putText(frame, text, (x, y - 5),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    #no_of_violators,result = FindCloserThanThreshold(points,threshold = 50)        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #cv2.putText(frame,\"No of people \"+str(no_of_people),(10,20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),3)\n",
    "    #cv2.putText(frame,\"No of Violators \"+str(no_of_violators),(10,70),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "    #cv2.imshow('Image',frame)\n",
    "    writer.write(frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
